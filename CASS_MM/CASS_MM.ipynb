{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import tarfile\n",
    "import tempfile\n",
    "import warnings\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import pandas_path  # Path style access for pandas\n",
    "from tqdm import tqdm\n",
    "import torch                    \n",
    "import torchvision\n",
    "import fasttext\n",
    "import pandas_path\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "torch.manual_seed(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd().parent / \"Data\" / \"hateful_memes\"\n",
    "img_tar_path = data_dir / \"img.tar.gz\"\n",
    "train_path = data_dir / \"train.jsonl\"\n",
    "dev_path = data_dir / \"dev_seen.jsonl\"\n",
    "test_path = data_dir / \"test_seen.jsonl.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path_frame = pd.read_json(dev_path, lines=True)\n",
    "dev_path_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_frame = pd.read_json(train_path, lines=True)\n",
    "train_samples_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_dfs(path):\n",
    "    dataframe = pd.read_json(path,lines=True)\n",
    "    max_id = dataframe[\"id\"].max() + 1 if not False else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val=make_train_valid_dfs(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset features Image, Caption and Labels \n",
    "\"\"\"\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, captions,labels, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.labels = list(labels)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=200\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "        label = torch.Tensor([list(self.labels)]).long().squeeze()\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "        item['label'] = self.labels[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "\n",
    "\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "transforms=A.Compose([A.Resize(384, 384, always_apply=True),\n",
    "                      A.Normalize(max_pixel_value=255.0, always_apply=True),])\n",
    "dataset = Dataset(train[\"img\"].values,train[\"text\"].values,train[\"label\"].values,tokenizer=tokenizer,transforms=transforms,)\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=4,num_workers=3,shuffle=True)\n",
    "\n",
    "val_dataset = Dataset(val[\"img\"].values,val[\"text\"].values,val[\"label\"].values,tokenizer=tokenizer,transforms=transforms,)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=4,num_workers=3,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../Data/hateful_memes\"\n",
    "captions_path = \"../Data/hateful_memes\"\n",
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = image_path\n",
    "    captions_path = captions_path\n",
    "    batch_size = 32\n",
    "    num_workers = 2\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_name = 'resnet50'\n",
    "    image_embedding = 2048\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 384\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Text encoding using BERT model\n",
    "\"\"\"\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageAndVisionConcat(torch.nn.Module):\n",
    "    def __init__(self,language_module,vision_module,\n",
    "                 language_feature_dim=300,vision_feature_dim=300,fusion_output_size=256,dropout_p=0.1,num_classes=1000):\n",
    "        super(LanguageAndVisionConcat, self).__init__()\n",
    "        self.language_module = language_module\n",
    "        self.language_adder=nn.Linear(768,300)\n",
    "        self.vision_module = vision_module\n",
    "        self.vision_adder=nn.Linear(512,300)\n",
    "        self.fusion = torch.nn.Linear(\n",
    "            in_features=(language_feature_dim + vision_feature_dim), \n",
    "            out_features=fusion_output_size\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(\n",
    "            in_features=fusion_output_size, \n",
    "            out_features=num_classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        language_op1=self.language_module(x['input_ids'].cuda(),x['attention_mask'])\n",
    "        language_op = self.language_adder(language_op1)\n",
    "        \n",
    "        vision_op1=self.vision_module(x['image'])\n",
    "        vision_op = self.vision_adder(vision_op1)\n",
    "        combined = torch.cat(\n",
    "            [language_op, vision_op], dim=1\n",
    "        )\n",
    "        fused = torch.nn.functional.relu(\n",
    "            self.fusion(combined)\n",
    "            )\n",
    "        \n",
    "        logits = self.fc(fused)\n",
    "        #pred = torch.nn.functional.softmax(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = LanguageAndVisionConcat(language_module=TextEncoder(),vision_module=timm.create_model('resnet50',  True, num_classes=512)).cuda()\n",
    "model_vit = LanguageAndVisionConcat(language_module=TextEncoder(),vision_module=timm.create_model('vit_base_patch16_384',  True,num_classes=512)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcontrib.optim import SWA\n",
    "params_cnn = [\n",
    "        {\"params\": model_cnn.vision_module.parameters(), \"lr\": CFG.image_encoder_lr},\n",
    "        {\"params\": model_cnn.language_module.parameters(), \"lr\": CFG.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(\n",
    "            model_cnn.vision_adder.parameters(), model_cnn.language_adder.parameters()\n",
    "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
    "    ]\n",
    "optimizer_cnn = torch.optim.AdamW(params_cnn, weight_decay=0.)\n",
    "\n",
    "params_vit = [\n",
    "        {\"params\": model_vit.vision_module.parameters(), \"lr\": CFG.image_encoder_lr},\n",
    "        {\"params\": model_vit.language_module.parameters(), \"lr\": CFG.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(\n",
    "            model_vit.vision_adder.parameters(), model_vit.language_adder.parameters()\n",
    "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
    "    ]\n",
    "optimizer_vit = torch.optim.AdamW(params_vit, weight_decay=0.)\n",
    "\n",
    "\n",
    "scheduler_cnn = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_cnn, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
    "    )\n",
    "scheduler_vit = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_vit, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    x_noise=(torch.normal(10e-6, 10e-9, size=(x.size()))).to(device)\n",
    "    x=x+x_noise\n",
    "    x =  torch.nn.functional.normalize(x, dim=-1, p=2)\n",
    "    y_noise=(torch.normal(10e-10, 10e-15, size=(y.size()))).to(device)\n",
    "    y=y+y_noise\n",
    "    y =  torch.nn.functional.normalize(y, dim=-1, p=2)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def ssl_train_model(train_loader,model_vit,optimizer_vit,scheduler_vit,model_cnn,optimizer_cnn,scheduler_cnn,num_epochs):\n",
    "    writer = SummaryWriter()\n",
    "    phase = 'train'\n",
    "    model_cnn.train()\n",
    "    model_vit.train()\n",
    "    f1_score_vit=0\n",
    "    best_loss=math.inf\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        #tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "        total_loss=0\n",
    "        for batch in train_loader:\n",
    "            optimizer_cnn.zero_grad()\n",
    "            optimizer_vit.zero_grad()\n",
    "            batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
    "            pred_vit = model_vit(batch)\n",
    "            pred_cnn = model_cnn(batch)\n",
    "            model_sim_loss=loss_fn(pred_cnn,pred_vit)\n",
    "            loss = model_sim_loss.mean()\n",
    "            loss.backward()\n",
    "            optimizer_cnn.step()\n",
    "            optimizer_vit.step()\n",
    "            scheduler_cnn.step(loss)\n",
    "            scheduler_vit.step(loss)\n",
    "            total_loss+=loss.item()\n",
    "        print('For -',i,'Loss:',total_loss)\n",
    "        if total_loss<best_loss:\n",
    "            best_loss=total_loss\n",
    "            print(\"Saving!\")\n",
    "            torch.save(model_cnn,'./vitb16-r50-CNNPART-CASS-BERT-384-logits-v2-optimparams-100-nodp-agg.pt')\n",
    "            torch.save(model_vit,'./vitb16-r50-VITPART-CASS-BERT-384-logits-v2-optimparams-100-nodp-agg.pt')\n",
    "    \n",
    "        writer.add_scalar(\"Self-Supervised Loss/train\", total_loss, i)\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "print('Training CASS multimodally')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ssl_train_model(dataloader,model_vit,optimizer_vit,scheduler_vit,model_cnn,optimizer_cnn,scheduler_cnn,num_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pranav_oct",
   "language": "python",
   "name": "pranav_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}