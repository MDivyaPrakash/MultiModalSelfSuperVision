# Understanding multi-modal self-supervision and out of distribution performance
Expanding Cross-Architectural Self-supervision for Multi-modal learning

## Abstract
Memes have become an online phenomenon and have grown
in circulation with increased accessibility to the internet and
editorial applications. This floods the internet with many
memes and makes direct human supervision impossible. A
plausible solution is to use AI to determine whether the
memes are harmful. But sometimes, these memes subtly com-
bine visual and textual cues to point to hate speech. Tra-
ditional deep learning architectures and techniques rely on
uni-modality i.e.; they only focus on images or textual data.
But as mentioned earlier, sometimes memes use a combi-
nation of textual and visual cues for hate speech; this calls
for deep learning techniques to be multi-modal in their ap-
proach. Furthermore, human intelligence also relies on mul-
tiple modalities. Another bottleneck is in the form of the
availability of annotations. As supervised techniques rely on
input-label pairs to learn meaningful representations, scal-
ing them is arduous and expensive. This is where we could
depend on self-supervised learning to understand represen-
tations label-freely. So taking the above problem as the in-
ception point, in this report, we have implemented two uni-
modal and two multimodal self-supervised approaches. In ad-
dition, we extend the implementation of CASS to multiple
modalities and suggest a novel multimodal self-supervised
technique CASS-MM (CASS- Multi-modal).

## Methodolgies Explored
-CLIP
-DINO
-CASS


