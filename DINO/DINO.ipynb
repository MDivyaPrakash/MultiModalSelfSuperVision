{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import tarfile\n",
    "import tempfile\n",
    "import warnings\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import pandas_path  # Path style access for pandas\n",
    "from tqdm import tqdm\n",
    "import torch                    \n",
    "import torchvision\n",
    "import fasttext\n",
    "import pandas_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd().parent / \"data\" / \"final\" / \"public\"\n",
    "\n",
    "img_tar_path = data_dir / \"img.tar.gz\"\n",
    "train_path = data_dir / \"train.jsonl\"\n",
    "dev_path = data_dir / \"dev.jsonl\"\n",
    "test_path = data_dir / \"test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd().parent / \"Data\" / \"hateful_memes\"\n",
    "img_tar_path = data_dir / \"img.tar.gz\"\n",
    "train_path = data_dir / \"train.jsonl\"\n",
    "dev_path = data_dir / \"dev_seen.jsonl\"\n",
    "test_path = data_dir / \"test_seen.jsonl.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path_frame = pd.read_json(dev_path, lines=True)\n",
    "dev_path_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_frame = pd.read_json(train_path, lines=True)\n",
    "train_samples_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_frame.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_frame.text.map(\n",
    "    lambda text: len(text.split(\" \"))\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "images = [\n",
    "    Image.open(\n",
    "        data_dir / train_samples_frame.loc[i, \"img\"]\n",
    "    ).convert(\"RGB\")\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "for image in images:\n",
    "    print(image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a callable image_transform with Compose\n",
    "image_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(size=(224, 224)),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# convert the images and prepare for visualization.\n",
    "tensor_img = torch.stack(\n",
    "    [image_transform(image) for image in images]\n",
    ")\n",
    "grid = torchvision.utils.make_grid(tensor_img)\n",
    "\n",
    "# plot\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.axis('off')\n",
    "_ = plt.imshow(grid.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook's Hateful meme Dataset features, Id, Image, Text and Label \n",
    "class HatefulMemesDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Uses jsonl data to preprocess and serve \n",
    "    dictionary of multimodal tensors for model input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        img_dir,\n",
    "        image_transform,\n",
    "        text_transform,\n",
    "        balance=False,\n",
    "        dev_limit=None,\n",
    "        random_state=0,\n",
    "    ):\n",
    "\n",
    "        self.samples_frame = pd.read_json(\n",
    "            data_path, lines=True\n",
    "        )\n",
    "        self.dev_limit = dev_limit\n",
    "        if balance:\n",
    "            neg = self.samples_frame[\n",
    "                self.samples_frame.label.eq(0)\n",
    "            ]\n",
    "            pos = self.samples_frame[\n",
    "                self.samples_frame.label.eq(1)\n",
    "            ]\n",
    "            self.samples_frame = pd.concat(\n",
    "                [\n",
    "                    neg.sample(\n",
    "                        pos.shape[0], \n",
    "                        random_state=random_state\n",
    "                    ), \n",
    "                    pos\n",
    "                ]\n",
    "            )\n",
    "        if self.dev_limit:\n",
    "            if self.samples_frame.shape[0] > self.dev_limit:\n",
    "                self.samples_frame = self.samples_frame.sample(\n",
    "                    dev_limit, random_state=random_state\n",
    "                )\n",
    "        self.samples_frame = self.samples_frame.reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        self.samples_frame.img = self.samples_frame.apply(\n",
    "            lambda row: (img_dir / row.img), axis=1\n",
    "        )\n",
    "\n",
    "        # https://github.com/drivendataorg/pandas-path\n",
    "        \"\"\"if not self.samples_frame.img.path.exists().all():\n",
    "            raise FileNotFoundError\n",
    "        if not self.samples_frame.img.path.is_file().all():\n",
    "            raise TypeError\"\"\"\n",
    "            \n",
    "        self.image_transform = image_transform\n",
    "        self.text_transform = text_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"This method is called when you do len(instance) \n",
    "        for an instance of this class.\n",
    "        \"\"\"\n",
    "        return len(self.samples_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"This method is called when you do instance[key] \n",
    "        for an instance of this class.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_id = self.samples_frame.loc[idx, \"id\"]\n",
    "\n",
    "        image = Image.open(\n",
    "            self.samples_frame.loc[idx, \"img\"]\n",
    "        ).convert(\"RGB\")\n",
    "        image = self.image_transform(image)\n",
    "\n",
    "        text = torch.Tensor(\n",
    "            self.text_transform.get_sentence_vector(\n",
    "                self.samples_frame.loc[idx, \"text\"]\n",
    "            )\n",
    "        ).squeeze()\n",
    "\n",
    "        if \"label\" in self.samples_frame.columns:\n",
    "            label = torch.Tensor(\n",
    "                [self.samples_frame.loc[idx, \"label\"]]\n",
    "            ).long().squeeze()\n",
    "            sample = {\n",
    "                \"id\": img_id, \n",
    "                \"image\": image, \n",
    "                \"text\": text, \n",
    "                \"label\": label\n",
    "            }\n",
    "        else:\n",
    "            sample = {\n",
    "                \"id\": img_id, \n",
    "                \"image\": image, \n",
    "                \"text\": text\n",
    "            }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageAndVisionConcat(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        loss_fn,\n",
    "        language_module,\n",
    "        vision_module,\n",
    "        language_feature_dim,\n",
    "        vision_feature_dim,\n",
    "        fusion_output_size,\n",
    "        dropout_p,\n",
    "        \n",
    "    ):\n",
    "        super(LanguageAndVisionConcat, self).__init__()\n",
    "        self.language_module = language_module\n",
    "        self.vision_module = vision_module\n",
    "        self.fusion = torch.nn.Linear(\n",
    "            in_features=(language_feature_dim + vision_feature_dim), \n",
    "            out_features=fusion_output_size\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(\n",
    "            in_features=fusion_output_size, \n",
    "            out_features=num_classes\n",
    "        )\n",
    "        self.loss_fn = loss_fn\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, text, image, label=None):\n",
    "        text_features = torch.nn.functional.relu(\n",
    "            self.language_module(text)\n",
    "        )\n",
    "        image_features = torch.nn.functional.relu(\n",
    "            self.vision_module(image)\n",
    "        )\n",
    "        combined = torch.cat(\n",
    "            [text_features, image_features], dim=1\n",
    "        )\n",
    "        fused = self.dropout(\n",
    "            torch.nn.functional.relu(\n",
    "            self.fusion(combined)\n",
    "            )\n",
    "        )\n",
    "        logits = self.fc(fused)\n",
    "        pred = torch.nn.functional.softmax(logits)\n",
    "        loss = (\n",
    "            self.loss_fn(pred, label) \n",
    "            if label is not None else label\n",
    "        )\n",
    "        return (pred, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "# for the purposes of this post, we'll filter\n",
    "# much of the lovely logging info from our LightningModule\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "class HatefulMemesModel(pl.LightningModule):\n",
    "    def __init__(self, hyperparams):\n",
    "        for data_key in [\"train_path\", \"dev_path\", \"img_dir\",]:\n",
    "            # ok, there's one for-loop but it doesn't count\n",
    "            if data_key not in hyperparams.keys():\n",
    "                raise KeyError(\n",
    "                    f\"{data_key} is a required hparam in this model\"\n",
    "                )\n",
    "        super(HatefulMemesModel, self).__init__()\n",
    "        self.hyperparams = hyperparams\n",
    "        # assign some hparams that get used in multiple places\n",
    "        self.embedding_dim = self.hyperparams.get(\"embedding_dim\", 300)\n",
    "        self.language_feature_dim = self.hyperparams.get(\n",
    "            \"language_feature_dim\", 300\n",
    "        )\n",
    "        self.vision_feature_dim = self.hyperparams.get(\n",
    "            # balance language and vision features by default\n",
    "            \"vision_feature_dim\", self.language_feature_dim\n",
    "        )\n",
    "        self.output_path = Path(\n",
    "            self.hyperparams.get(\"output_path\", \"model-outputs\")\n",
    "        )\n",
    "        self.output_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # instantiate transforms, datasets\n",
    "        self.text_transform = self._build_text_transform()\n",
    "        self.image_transform = self._build_image_transform()\n",
    "        self.train_dataset = self._build_dataset(\"train_path\")\n",
    "        self.dev_dataset = self._build_dataset(\"dev_path\")\n",
    "        \n",
    "        # set up model and training\n",
    "        self.model = self._build_model()\n",
    "        self.trainer_params = self._get_trainer_params()\n",
    "    \n",
    "    ## Required LightningModule Methods (when validating) ##\n",
    "    \n",
    "    def forward(self, text, image, label=None):\n",
    "        return self.model(text, image, label)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        preds, loss = self.forward(\n",
    "            text=batch[\"text\"], \n",
    "            image=batch[\"image\"], \n",
    "            label=batch[\"label\"]\n",
    "        )\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        preds, loss = self.eval().forward(\n",
    "            text=batch[\"text\"], \n",
    "            image=batch[\"image\"], \n",
    "            label=batch[\"label\"]\n",
    "        )\n",
    "        \n",
    "        return {\"batch_val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack(\n",
    "            tuple(\n",
    "                output[\"batch_val_loss\"] \n",
    "                for output in outputs\n",
    "            )\n",
    "        ).mean()\n",
    "        \n",
    "        return {\n",
    "            \"val_loss\": avg_loss,\n",
    "            \"progress_bar\":{\"avg_val_loss\": avg_loss}\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizers = torch.optim.AdamW(\n",
    "                self.model.parameters(), \n",
    "                lr=self.hyperparams.get(\"lr\", 0.001))\n",
    "        \n",
    "        schedulers = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizers)\n",
    "        \n",
    "        \n",
    "        monitor = 'val_loss'\n",
    "        #return optimizers, schedulers ,monitor\n",
    "        return {\n",
    "           'optimizer': optimizers,\n",
    "           'scheduler': schedulers,\n",
    "           'monitor': 'val_loss'\n",
    "       }\n",
    "    \n",
    "    #@pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset, \n",
    "            shuffle=True, \n",
    "            batch_size=self.hyperparams.get(\"batch_size\", 4), \n",
    "            num_workers=self.hyperparams.get(\"num_workers\", 16)\n",
    "        )\n",
    "\n",
    "    #@pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.dev_dataset, \n",
    "            shuffle=False, \n",
    "            batch_size=self.hyperparams.get(\"batch_size\", 4), \n",
    "            num_workers=self.hyperparams.get(\"num_workers\", 16)\n",
    "        )\n",
    "    \n",
    "    ## Convenience Methods ##\n",
    "    \n",
    "    def fit(self):\n",
    "        self._set_seed(self.hyperparams.get(\"random_state\", 42))\n",
    "        self.trainer = pl.Trainer(**self.trainer_params)\n",
    "        self.trainer.fit(self)\n",
    "        \n",
    "    def _set_seed(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def _build_text_transform(self):\n",
    "        with tempfile.NamedTemporaryFile() as ft_training_data:\n",
    "            ft_path = Path(ft_training_data.name)\n",
    "            with ft_path.open(\"w\") as ft:\n",
    "                training_data = [\n",
    "                    json.loads(line)[\"text\"] + \"/n\" \n",
    "                    for line in open(\n",
    "                        self.hyperparams.get(\"train_path\")\n",
    "                    ).read().splitlines()\n",
    "                ]\n",
    "                for line in training_data:\n",
    "                    ft.write(line + \"\\n\")\n",
    "                language_transform = fasttext.train_unsupervised(\n",
    "                    str(ft_path),\n",
    "                    model=self.hyperparams.get(\"fasttext_model\", \"cbow\"),\n",
    "                    dim=self.embedding_dim\n",
    "                )\n",
    "        return language_transform\n",
    "    \n",
    "    def _build_image_transform(self):\n",
    "        image_dim = self.hyperparams.get(\"image_dim\", 224)\n",
    "        image_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize(\n",
    "                    size=(image_dim, image_dim)\n",
    "                ),        \n",
    "                torchvision.transforms.ToTensor(),\n",
    "                # all torchvision models expect the same\n",
    "                # normalization mean and std\n",
    "                # https://pytorch.org/docs/stable/torchvision/models.html\n",
    "                torchvision.transforms.Normalize(\n",
    "                    mean=(0.485, 0.456, 0.406), \n",
    "                    std=(0.229, 0.224, 0.225)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        return image_transform\n",
    "\n",
    "    def _build_dataset(self, dataset_key):\n",
    "        return HatefulMemesDataset(\n",
    "            data_path=self.hyperparams.get(dataset_key, dataset_key),\n",
    "            img_dir=self.hyperparams.get(\"img_dir\"),\n",
    "            image_transform=self.image_transform,\n",
    "            text_transform=self.text_transform,\n",
    "            # limit training samples only\n",
    "            dev_limit=(\n",
    "                self.hyperparams.get(\"dev_limit\", None) \n",
    "                if \"train\" in str(dataset_key) else None\n",
    "            ),\n",
    "            balance=True if \"train\" in str(dataset_key) else False,\n",
    "        )\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # we're going to pass the outputs of our text\n",
    "        # transform through an additional trainable layer\n",
    "        # rather than fine-tuning the transform\n",
    "        language_module = torch.nn.Linear(\n",
    "                in_features=self.embedding_dim,\n",
    "                out_features=self.language_feature_dim\n",
    "        )\n",
    "        \n",
    "        # easiest way to get features rather than\n",
    "        # classification is to overwrite last layer\n",
    "        # with an identity transformation, we'll reduce\n",
    "        # dimension using a Linear layer, resnet is 2048 out\n",
    "        vision_module = torch.load('DINO-r50-224-hm.pt')\n",
    "        #timm.create_model('resnet50', pretrained=True)\n",
    "        vision_module.fc = torch.nn.Linear(\n",
    "                in_features=2048,\n",
    "                out_features=self.vision_feature_dim\n",
    "        )\n",
    "\n",
    "        return LanguageAndVisionConcat(\n",
    "            num_classes=self.hyperparams.get(\"num_classes\", 2),\n",
    "            loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "            language_module=language_module,\n",
    "            vision_module=vision_module,\n",
    "            language_feature_dim=self.language_feature_dim,\n",
    "            vision_feature_dim=self.vision_feature_dim,\n",
    "            fusion_output_size=self.hyperparams.get(\n",
    "                \"fusion_output_size\", 512\n",
    "            ),\n",
    "            dropout_p=self.hyperparams.get(\"dropout_p\", 0.1),\n",
    "        )\n",
    "    \n",
    "    def _get_trainer_params(self):\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=self.output_path,\n",
    "            monitor=self.hyperparams.get(\n",
    "                \"checkpoint_monitor\", \"avg_val_loss\"\n",
    "            ),\n",
    "            mode=self.hyperparams.get(\n",
    "                \"checkpoint_monitor_mode\", \"min\"\n",
    "            ),\n",
    "            verbose=self.hyperparams.get(\"verbose\", True)\n",
    "        )\n",
    "\n",
    "        early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "            monitor=self.hyperparams.get(\n",
    "                \"early_stop_monitor\", \"avg_val_loss\"\n",
    "            ),\n",
    "            min_delta=self.hyperparams.get(\n",
    "                \"early_stop_min_delta\", 0.001\n",
    "            ),\n",
    "            patience=self.hyperparams.get(\n",
    "                \"early_stop_patience\", 3\n",
    "            ),\n",
    "            verbose=self.hyperparams.get(\"verbose\", True),\n",
    "        )\n",
    "\n",
    "        trainer_params = {\n",
    "            #\"checkpoint_callback\": checkpoint_callback,\n",
    "            #\"early_stop_callback\": early_stop_callback,\n",
    "            #\"default_save_path\": self.output_path,\n",
    "            \"accumulate_grad_batches\": self.hyperparams.get(\n",
    "                \"accumulate_grad_batches\", 1\n",
    "            ),\n",
    "            \"gpus\": self.hyperparams.get(\"n_gpu\", 1),\n",
    "            \"max_epochs\": self.hyperparams.get(\"max_epochs\", 100),\n",
    "            \"gradient_clip_val\": self.hyperparams.get(\n",
    "                \"gradient_clip_value\", 1\n",
    "            ),\n",
    "        }\n",
    "        return trainer_params\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def make_submission_frame(self, test_path):\n",
    "        test_dataset = self._build_dataset(test_path)\n",
    "        submission_frame = pd.DataFrame(\n",
    "            index=test_dataset.samples_frame.id,\n",
    "            columns=[\"proba\", \"label\"]\n",
    "        )\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_dataset, \n",
    "            shuffle=False, \n",
    "            batch_size=self.hyperparams.get(\"batch_size\", 4), \n",
    "            num_workers=self.hyperparams.get(\"num_workers\", 16))\n",
    "        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "            preds, _ = self.model.eval().to(\"cpu\")(\n",
    "                batch[\"text\"], batch[\"image\"]\n",
    "            )\n",
    "            submission_frame.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n",
    "            submission_frame.loc[batch[\"id\"], \"label\"] = preds.argmax(dim=1)\n",
    "        submission_frame.proba = submission_frame.proba.astype(float)\n",
    "        submission_frame.label = submission_frame.label.astype(int)\n",
    "        return submission_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    # Required hparams\n",
    "    \"train_path\": train_path,\n",
    "    \"dev_path\": dev_path,\n",
    "    \"img_dir\": data_dir,\n",
    "    # Optional hparams\n",
    "    \"embedding_dim\": 150,\n",
    "    \"language_feature_dim\": 300,\n",
    "    \"vision_feature_dim\": 300,\n",
    "    \"fusion_output_size\": 256,\n",
    "    \"output_path\": \"model-cass-R50\",\n",
    "    \"dev_limit\": None,\n",
    "    \"lr\": 0.00005,\n",
    "    \"max_epochs\": 50,\n",
    "    \"n_gpu\": 1,\n",
    "    \"batch_size\": 4,\n",
    "    # allows us to \"simulate\" having larger batches \n",
    "    \"accumulate_grad_batches\": 16,\n",
    "    \"early_stop_patience\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hateful_memes_model = HatefulMemesModel(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hateful_memes_model.make_submission_frame(test_seen_path).to_csv('untrained_test_seen_path.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hateful_memes_model.make_submission_frame(test_unseen_path).to_csv('untrained_test_unseen_path.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hateful_memes_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seen_path = data_dir / \"test_seen.jsonl\"\n",
    "test_unseen_path = data_dir / \"test_unseen.jsonl\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hateful_memes_model.make_submission_frame(test_seen_path).to_csv('r50_dino_224_trained_test_seen_path.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hateful_memes_model.make_submission_frame(test_unseen_path).to_csv('r50_dino_224_trained_test_unseen_path.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pranav_oct",
   "language": "python",
   "name": "pranav_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
