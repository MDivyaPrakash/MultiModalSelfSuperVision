{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import tarfile\n",
    "import tempfile\n",
    "import warnings\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import pandas_path  # Path style access for pandas\n",
    "from tqdm import tqdm\n",
    "import torch                    \n",
    "import torchvision\n",
    "import fasttext\n",
    "import pandas_path\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the path for dataset and json files accompanying the dataset\n",
    "data_dir = Path.cwd().parent / \"Data\" / \"hateful_memes\"\n",
    "img_tar_path = data_dir / \"img.tar.gz\"\n",
    "train_path = data_dir / \"train.jsonl\"\n",
    "dev_path = data_dir / \"dev_seen.jsonl\"\n",
    "test_path = data_dir / \"test_seen.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path_frame = pd.read_json(dev_path, lines=True)\n",
    "dev_path_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_frame = pd.read_json(train_path, lines=True)\n",
    "train_samples_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the DistilBERT tokenizer.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_dfs(path):\n",
    "    dataframe = pd.read_json(path,lines=True)\n",
    "    max_id = dataframe[\"id\"].max() + 1 if not False else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42) #using the same seed value to make sure we train on the same training set across differnt approaches. \n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val=make_train_valid_dfs(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, captions,labels, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.labels = list(labels)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=200\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "        label = torch.Tensor([list(self.labels)]).long().squeeze()\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "        item['label'] = self.labels[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "transforms=A.Compose([A.Resize(384, 384, always_apply=True),\n",
    "                      A.Normalize(max_pixel_value=255.0, always_apply=True),])\n",
    "dataset = CLIPDataset(train[\"img\"].values,train[\"text\"].values,train[\"label\"].values,tokenizer=tokenizer,transforms=transforms,)\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=4,num_workers=3,shuffle=True)\n",
    "\n",
    "val_dataset = CLIPDataset(val[\"img\"].values,val[\"text\"].values,val[\"label\"].values,tokenizer=tokenizer,transforms=transforms,)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=4,num_workers=3,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/scratch/ps4364/HM/Data/hateful_memes\"\n",
    "captions_path = \"/scratch/ps4364/HM/Data/hateful_memes\"\n",
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = image_path\n",
    "    captions_path = captions_path\n",
    "    batch_size = 32\n",
    "    num_workers = 2\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_name = 'resnet50'\n",
    "    image_embedding = 2048\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 384\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1\n",
    "    \n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageAndVisionConcat(torch.nn.Module):\n",
    "    def __init__(self,language_module,vision_module,\n",
    "                 language_feature_dim=300,vision_feature_dim=300,fusion_output_size=512,dropout_p=0.1,num_classes=2):\n",
    "        super(LanguageAndVisionConcat, self).__init__()\n",
    "        self.language_module = language_module\n",
    "        self.language_adder=nn.Linear(768,300)\n",
    "        self.vision_module = vision_module\n",
    "        self.vision_adder=nn.Linear(512,300)\n",
    "        self.fusion = torch.nn.Linear(\n",
    "            in_features=(language_feature_dim + vision_feature_dim), \n",
    "            out_features=fusion_output_size\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(\n",
    "            in_features=fusion_output_size, \n",
    "            out_features=num_classes\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        language_op1=model1.text_encoder(x['input_ids'].cuda(),x['attention_mask'])\n",
    "        language_op = self.language_adder(language_op1)\n",
    "        \n",
    "        vision_op1=model1.image_encoder(x['image'])\n",
    "        vision_op = self.vision_adder(vision_op1)\n",
    "        combined = torch.cat(\n",
    "            [language_op, vision_op], dim=1\n",
    "        )\n",
    "        fused = self.dropout(\n",
    "            torch.nn.functional.relu(\n",
    "            self.fusion(combined)\n",
    "            )\n",
    "        )\n",
    "        logits = self.fc(fused)\n",
    "        pred = torch.nn.functional.softmax(logits)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=torch.load('./vitb16-r50-CNNPART-CASS-BERT-384-logits-FINAL.pt')\n",
    "#Loading the pretrained model that we would be finetuning with full-supervision. \n",
    "model = LanguageAndVisionConcat(model1.text_encoder,model1.image_encoder).cuda()\n",
    "optimizer = torch.optim.AdamW(\n",
    "                model.parameters(), \n",
    "                lr=0.00005)\n",
    "        \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "from sklearn.metrics import *\n",
    "import math\n",
    "def train(model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    loss_fn,epochs):\n",
    "    best_train_loss=math.inf\n",
    "    best_val_loss=math.inf\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        labels=[]\n",
    "        predictions=[]\n",
    "        model.train()\n",
    "        total_loss=0\n",
    "        for x in dataloader:\n",
    "            batch = {k: v.to(CFG.device) for k, v in x.items() if k != \"caption\"}\n",
    "            output=model(batch)\n",
    "            loss = loss_fn(output,batch['label'])\n",
    "            output=torch.argmax(output,dim=1)\n",
    "            labels.append(batch['label'].cpu().detach().tolist())\n",
    "            predictions.append(output.cpu().detach().tolist())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss+=loss.item()\n",
    "        total_loss=total_loss/len(dataloader)\n",
    "        fpr, tpr, thresholds=roc_curve(np.asarray(labels[0]),np.asarray(predictions[0]))\n",
    "        train_roc=auc(fpr, tpr)\n",
    "        current_train_acc=accuracy_score(np.asarray(labels[0]),np.asarray(predictions[0]))\n",
    "        print('For this epoch the loss was:',total_loss,'AUC value:',train_roc,'And acc:',current_train_acc)\n",
    "        if best_train_loss>total_loss:\n",
    "            #Run this loop only when new loss is less than the previosu minimoum loss \n",
    "            best_train_loss=total_loss\n",
    "            #update the loss value to a new minimum\n",
    "            print('Validating!')\n",
    "            model.eval()\n",
    "            val_loss=0\n",
    "            val_labels=[]\n",
    "            val_predictions=[]\n",
    "            for x in val_dataloader:\n",
    "                batch = {k: v.to(CFG.device) for k, v in x.items() if k != \"caption\"}\n",
    "                output=model(batch)\n",
    "                val_loss = loss_fn(output,batch['label'].cuda())\n",
    "                val_output=torch.argmax(output,dim=1)\n",
    "                val_labels.append(batch['label'].cpu().detach().tolist())\n",
    "                val_predictions.append(val_output.cpu().detach().tolist())\n",
    "                val_loss+=val_loss.item()\n",
    "            val_loss=val_loss/len(val_dataloader)\n",
    "            fpr, tpr, thresholds=roc_curve(np.asarray(val_labels[0]),np.asarray(val_predictions[0]))\n",
    "            val_roc=auc(fpr, tpr)\n",
    "            current_val_acc=accuracy_score(np.asarray(val_labels[0]),np.asarray(val_predictions[0]))\n",
    "            print('For this epoch the loss was:',val_loss,'AUC value:',val_roc,'And acc:',current_val_acc)\n",
    "            if best_val_loss>val_loss:\n",
    "                #Run this loop only when we have a new minimum for validation loss.\n",
    "                best_val_loss=val_loss\n",
    "                #update the loss to reflect the new minimum.\n",
    "                print('Saving Model!')\n",
    "                torch.save(model,'./vitb16-r50-CNNPART-CASS-BERT-384-logits-FINAL-FT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "train(model,optimizer,optimizer,loss_fn,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pranav_oct",
   "language": "python",
   "name": "pranav_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}